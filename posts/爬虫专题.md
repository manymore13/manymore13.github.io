## Requests


```bash
#本人电脑环境
Name: requests
Version: 2.30.0
Summary: Python HTTP for Humans.
Home-page: https://requests.readthedocs.io
Author: Kenneth Reitz
Author-email: me@kennethreitz.org
License: Apache 2.0

```
具体用法参考 [Home-page](https://requests.readthedocs.io/en/latest/)


## Scrapy
- [scrapy官网](https://docs.scrapy.org/)
- [scrapy-cookbook](https://scrapy-cookbook.readthedocs.io/zh_CN/latest/index.html)



## selenium
---
>Selenium是一个自动化测试工具，支持各种浏览器，包括Chrome、Safari、Firefox 等主流界面式浏览器。简单理解，Selenium可以模拟操作浏览器，对一些需要动态加载的页面，不需要我们执行JavaScript等操作，即可自动加载完成后的页面。

### 安装Selenium
直接使用pip安装
```bash
pip install selenium
```
1. 安装Selenium之后，还需要下载对应的浏览器驱动
| [Chrome driver](https://sites.google.com/a/chromium.org/chromedriver/home) | [Firefox driver](https://github.com/mozilla/geckodriver/releases) | [IE driver](https://github.com/mozilla/geckodriver/releases) | [Edge driver](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver) |
|----------------------------------------------------------------------------|-------------------------------------------------------------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------|

2. 检查是否安装成功
```python
from selenium import webdriver
driver = webdriver.Edge()
driver.get("https://www.so.com/")
input("按任意键退出")
driver.quit()
```
若能正常打开浏览器，则安装成功

参考链接
- [selenium官网](https://www.selenium.dev/documentation/)
- [selenium使用教程](https://pythondjango.cn/python/tools/7-python_selenium/)

## 案例分享
>分享一个简单的例子：爬静态网页，通过requests获取网页html，再通过正则匹配内容，最后把内容下载保存到本地

```python
import os
import re
from multiprocessing.dummy import Pool

import requests

start_url = 'https://www.kanunu8.com/book3/5932/'
dir_name = '李敖回忆录'


def get_html(url):
    """
    获取网页源码
    :param url:网址
    :return: 网页源代码
    """
    return requests.get(url).content.decode('gbk')


def get_all_chapter_link(html):
    """
    :param html:
    :return: 章节名草，章节链接
    """
    all_chapter_url = []
    chapter_block = re.findall('正文<(.*?)</tbody>', html, re.S)[0]
    chapter_url = re.findall('href="(.*?)">', chapter_block, re.S)
    for url in chapter_url:
        all_chapter_url.append(start_url + url)
    return all_chapter_url


def save_chapter_article(chapter_url):
    """
    :param chapter_url:
    :return: 章节名称，章节内容
    """
    chapter_html = get_html(chapter_url).replace('\n', '')
    chapter_name = re.findall('size="4">(.*?)</font>', chapter_html, re.S)[0]
    chapter_content = re.findall('<p>(.*?)</p>', chapter_html, re.S)[0]
    chapter_content = chapter_content.replace('<br />', '')
    save(chapter_name, chapter_content)


def save(chapter_name, chapter_content):
    """
    :param chapter_name: 章节名草
    :param chapter_content:  章节内容
    :return: None
    """
    os.makedirs(dir_name, exist_ok=True)
    with open(os.path.join(dir_name, chapter_name + '.txt'), 'w', encoding='utf-8') as f:
        f.write(chapter_content)


if __name__ == '__main__':
    start_html = get_html(start_url)
    all_chapter_url = get_all_chapter_link(start_html)
    pool = Pool(4)
    pool.map(save_chapter_article, all_chapter_url)
    
```
