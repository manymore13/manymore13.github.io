## Requests
Requests是一个优雅而简单的Python HTTP库。

具体用法参考 [Home-page](https://requests.readthedocs.io/en/latest/)

---
## Scrapy
Scrapy是一个快速的高级网页抓取和网页抓取框架，用于抓取网站并从页面中提取结构化数据。它广泛用于数据挖掘到监控和自动化测试领域。
- [scrapy官网](https://docs.scrapy.org/)
- [scrapy-cookbook](https://scrapy-cookbook.readthedocs.io/zh_CN/latest/index.html)

---
## Selenium
`Selenium`是一个自动化测试工具，支持各种浏览器，包括Chrome、Safari、Firefox 等主流界面式浏览器。简单理解，Selenium可以模拟操作浏览器，对一些需要动态加载的页面，不需要我们执行JavaScript等操作，即可自动加载完成后的页面。

---
### 安装Selenium
直接使用pip安装
```bash
pip install selenium
```
我安装的版本是 Version: 4.9.0

- 安装Selenium之后，还需要下载对应的浏览器驱动

| [Chrome driver](https://sites.google.com/a/chromium.org/chromedriver/home) | [Firefox driver](https://github.com/mozilla/geckodriver/releases) | [IE driver](https://github.com/mozilla/geckodriver/releases) | [Edge driver](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver) |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |

- 检查是否安装成功

```python
from selenium import webdriver
driver = webdriver.Edge()
driver.get("https://www.so.com/")
input("按任意键退出")
driver.quit()
```
若能正常打开浏览器，则安装成功

### 定位元素

```html
<ol id="vegetables">
 <li class="potatoes">…
 <li class="onions">…
 <li class="tomatoes"><span>Tomato is a Vegetable</span>…
</ol>
<ul id="fruits">
  <li class="bananas">…
  <li class="apples">…
  <li class="tomatoes"><span>Tomato is a Fruit</span>…
</ul>

```
以上面html为例，我们来定位元素
```python

from selenium import webdriver
from selenium.webdriver.common.by import By

# 通过class name定位
vegetable = driver.find_element(By.CLASS_NAME, "tomatoes")

# 通过id定位水果元素，再通过水果元素定位西红柿元素，这种嵌套定位效率不是最好的
fruits = driver.find_element(By.ID, "fruits")
fruit = fruits.find_element(By.CLASS_NAME,"tomatoes")

# 通过CSS 选择器定位元素
fruit = driver.find_element(By.CSS_SELECTOR,"#fruits .tomatoes")

# 通过标签名定位元素，这里是获取所有标签li   
plants = driver.find_elements(By.TAG_NAME, "li")


```

另外一个实例: 打开百度，然后搜索

```python
# 我的是微软Edge浏览器
driver = webdriver.Edge()
# 加载网页.
driver.get("https://www.baidu.com/")

title = driver.title
assert title == "百度一下，你就知道"

driver.implicitly_wait(0.5)
# 获取输入框元素
text_box = driver.find_element(by=By.ID, value="kw")
# 获取提交按钮元素
submit_button = driver.find_element(by=By.CSS_SELECTOR, value="#su")
# 输入关键词
text_box.send_keys("Selenium")
# 点击元素
submit_button.click()
# 暂停一下，再退出，否则代码走完程序都退出，浏览器会一闪而过
input("enter any key, quit")
# 关闭浏览器
driver.quit()
```


### 参考链接
- [selenium官网](https://www.selenium.dev/documentation/)
- [selenium使用教程](https://pythondjango.cn/python/tools/7-python_selenium/)




## 案例分享

分享一个简单的例子：爬静态网页，通过requests获取网页html，再通过正则匹配内容，最后把内容下载保存到本地


```python
import os
import re
from multiprocessing.dummy import Pool

import requests

start_url = 'https://www.kanunu8.com/book3/5932/'
dir_name = '李敖回忆录'


def get_html(url):
    """
    获取网页源码
    :param url:网址
    :return: 网页源代码
    """
    return requests.get(url).content.decode('gbk')


def get_all_chapter_link(html):
    """
    :param html:
    :return: 章节名草，章节链接
    """
    all_chapter_url = []
    chapter_block = re.findall('正文<(.*?)</tbody>', html, re.S)[0]
    chapter_url = re.findall('href="(.*?)">', chapter_block, re.S)
    for url in chapter_url:
        all_chapter_url.append(start_url + url)
    return all_chapter_url


def save_chapter_article(chapter_url):
    """
    :param chapter_url:
    :return: 章节名称，章节内容
    """
    chapter_html = get_html(chapter_url).replace('\n', '')
    chapter_name = re.findall('size="4">(.*?)</font>', chapter_html, re.S)[0]
    chapter_content = re.findall('<p>(.*?)</p>', chapter_html, re.S)[0]
    chapter_content = chapter_content.replace('<br />', '')
    save(chapter_name, chapter_content)


def save(chapter_name, chapter_content):
    """
    :param chapter_name: 章节名草
    :param chapter_content:  章节内容
    :return: None
    """
    os.makedirs(dir_name, exist_ok=True)
    with open(os.path.join(dir_name, chapter_name + '.txt'), 'w', encoding='utf-8') as f:
        f.write(chapter_content)


if __name__ == '__main__':
    start_html = get_html(start_url)
    all_chapter_url = get_all_chapter_link(start_html)
    pool = Pool(4)
    pool.map(save_chapter_article, all_chapter_url)
    
```
---

## 其他

- [Python3网络爬虫开发实战](https://python3webspider.cuiqingcai.com/)
---
